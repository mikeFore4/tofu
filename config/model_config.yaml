llama2-7b:
  hf_key: "NousResearch/Llama-2-7b-chat-hf"
  question_start_tag: "[INST] "
  question_end_tag: " [/INST]"
  answer_tag: ""
  answer_end_tag: ""
  flash_attention2: "true"
  gradient_checkpointing: "true"
  ft_model_path: "locuslab/tofu_ft_llama2-7b" #this model will be used for unlearning by default
llama3-8b:
  hf_key: "meta-llama/Meta-Llama-3-8B-Instruct"
  question_start_tag: "<|start_header_id|>user<|end_header_id|>\n\n"
  question_end_tag: "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
  answer_tag: ""
  answer_end_tag: "<|eot_id|>"
  flash_attention2: "true"
  gradient_checkpointing: "true"
  ft_model_path: "locuslab/tofu_ft_llama3-8b"
  rag_config:
    use_rag: False
    rag_start_token: ""
    rag_end_token: ""
    chromadb_path: ""
    collection_name: ""
    emb_model: "all-miniLM-L6-v2"
phi:
  hf_key: "microsoft/phi-1_5"
  question_start_tag: "Question: "
  question_end_tag: "\n"
  answer_tag: "Answer: "
  answer_end_tag: ""
  flash_attention2: "false"
  gradient_checkpointing: "false"
  ft_model_path: "locuslab/tofu_ft_phi-1.5"
stablelm:
  hf_key: "stabilityai/stablelm-3b-4e1t"
  question_start_tag: "Question: "
  question_end_tag: "\n"
  answer_tag: "Answer: "
  answer_end_tag: ""
  flash_attention2: "false"
  gradient_checkpointing: "false"
  ft_model_path: "paper_models/final_ft_noLORA_5_epochs_inst_lr1e-05_stablelm/checkpoint-625"
pythia-1.4:
  hf_key: "EleutherAI/pythia-1.4b-deduped"
  question_start_tag: "Question: "
  question_end_tag: "\n"
  answer_tag: "Answer: "
  answer_end_tag: ""
  flash_attention2: "false"
  gradient_checkpointing: "false"

  
